{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd105872-91bf-4128-94dd-06077794c116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Training data columns: ['id', 'Name', 'Gender', 'Age', 'City', 'Working Professional or Student', 'Profession', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Financial Stress', 'Family History of Mental Illness', 'Depression']\n",
      "Test data columns: ['id', 'Name', 'Gender', 'Age', 'City', 'Working Professional or Student', 'Profession', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Financial Stress', 'Family History of Mental Illness']\n",
      "\n",
      "Training model...\n",
      "[0]\tvalidation_0-logloss:0.47469\n",
      "[100]\tvalidation_0-logloss:0.25120\n",
      "[200]\tvalidation_0-logloss:0.19236\n",
      "[300]\tvalidation_0-logloss:0.17090\n",
      "[400]\tvalidation_0-logloss:0.16153\n",
      "[500]\tvalidation_0-logloss:0.15668\n",
      "[600]\tvalidation_0-logloss:0.15404\n",
      "[700]\tvalidation_0-logloss:0.15242\n",
      "[800]\tvalidation_0-logloss:0.15137\n",
      "[900]\tvalidation_0-logloss:0.15074\n",
      "[999]\tvalidation_0-logloss:0.15026\n",
      "\n",
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.96     22986\n",
      "         1.0       0.84      0.82      0.83      5154\n",
      "\n",
      "    accuracy                           0.94     28140\n",
      "   macro avg       0.90      0.89      0.90     28140\n",
      "weighted avg       0.94      0.94      0.94     28140\n",
      "\n",
      "Validation Accuracy: 0.9385\n",
      "\n",
      "Making predictions on test data...\n",
      "\n",
      "Submission file created: submission_xgboost.csv\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                                  feature  importance\n",
      "3         Working Professional or Student    0.357646\n",
      "1                                     Age    0.225247\n",
      "13  Have you ever had suicidal thoughts ?    0.192678\n",
      "5                       Academic Pressure    0.054049\n",
      "15                       Financial Stress    0.037184\n",
      "6                           Work Pressure    0.030962\n",
      "9                        Job Satisfaction    0.023358\n",
      "11                         Dietary Habits    0.019493\n",
      "14                       Work/Study Hours    0.016416\n",
      "10                         Sleep Duration    0.010577\n",
      "\n",
      "Model saved as: depression_prediction_xgboost.model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def encode_categorical(series, is_train=True, mapping=None):\n",
    "    if is_train:\n",
    "        # Create a mapping for unique values\n",
    "        unique_values = series.unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    \n",
    "    # Transform using mapping, assign max value + 1 for unseen categories\n",
    "    max_val = max(mapping.values()) if mapping else 0\n",
    "    return series.map(lambda x: mapping.get(x, max_val + 1)), mapping\n",
    "\n",
    "def preprocess_data(df, is_train=True, encoders=None):\n",
    "    # Create a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop ID and Name columns\n",
    "    cols_to_drop = ['id', 'Name']\n",
    "    if not is_train:\n",
    "        # For test data, also drop the Depression column if it exists\n",
    "        cols_to_drop.append('Depression')\n",
    "    df = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)\n",
    "    \n",
    "    # Initialize encoders dictionary if training\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        if column != 'Depression':\n",
    "            df[column], mapping = encode_categorical(\n",
    "                df[column].astype(str),\n",
    "                is_train=is_train,\n",
    "                mapping=encoders.get(column)\n",
    "            )\n",
    "            if is_train:\n",
    "                encoders[column] = mapping\n",
    "    \n",
    "    # Handle numeric variables\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Convert all columns to float\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    return df, encoders\n",
    "\n",
    "def train_model(train_data):\n",
    "    # Separate features and target\n",
    "    X = train_data.drop('Depression', axis=1)\n",
    "    y = train_data['Depression']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # XGBoost parameters (similar to original LightGBM parameters)\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'n_estimators': 2000,\n",
    "        'verbosity': 0,\n",
    "        'early_stopping_rounds': 40,\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    val_predictions = model.predict(X_val)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        train_data = pd.read_csv('train.csv')\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "        \n",
    "        # Print initial column names for debugging\n",
    "        print(\"\\nTraining data columns:\", train_data.columns.tolist())\n",
    "        print(\"Test data columns:\", test_data.columns.tolist())\n",
    "        \n",
    "        # Preprocess training data\n",
    "        processed_train, encoders = preprocess_data(train_data, is_train=True)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nTraining model...\")\n",
    "        model = train_model(processed_train)\n",
    "        \n",
    "        # Process test data using the same encoders\n",
    "        processed_test, _ = preprocess_data(test_data, is_train=False, encoders=encoders)\n",
    "        \n",
    "        # Ensure columns match between train and test\n",
    "        train_cols = processed_train.drop('Depression', axis=1).columns\n",
    "        processed_test = processed_test[train_cols]\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_predictions = model.predict(processed_test)\n",
    "        \n",
    "        # Create submission file\n",
    "        submission = pd.DataFrame({\n",
    "            'id': test_data['id'],\n",
    "            'Depression': test_predictions\n",
    "        })\n",
    "        submission.to_csv('submission7.csv', index=False)\n",
    "        print(\"\\nSubmission file created: submission_xgboost.csv\")\n",
    "        \n",
    "        # Print feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': train_cols,\n",
    "            'importance': model.feature_importances_\n",
    "        })\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "        \n",
    "        # Save model\n",
    "        model.save_model('depression_prediction_xgboost.model')\n",
    "        print(\"\\nModel saved as: depression_prediction_xgboost.model\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nDetailed error information:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27249f99-3883-48c6-a978-fe126187583f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
