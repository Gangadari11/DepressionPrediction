{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f660effa-d670-4e42-bbfa-f43f47d244c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2703949019.py, line 314)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 314\u001b[1;36m\u001b[0m\n\u001b[1;33m    S print(f\"An error occurred: {str(e)}\")\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def encode_categorical(series, is_train=True, mapping=None):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables by mapping each unique value to an integer.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: The categorical column to encode.\n",
    "    - is_train: If True, create a new mapping. If False, use the provided mapping.\n",
    "    - mapping: Dictionary mapping categorical values to integers (used for test data).\n",
    "    \n",
    "    Returns:\n",
    "    - Encoded series with integer values.\n",
    "    - The mapping dictionary (useful for encoding test data consistently).\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        # Create a mapping for unique values\n",
    "        unique_values = series.unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    \n",
    "    # Transform using mapping, assign max value + 1 for unseen categories\n",
    "    max_val = max(mapping.values()) if mapping else 0\n",
    "    return series.map(lambda x: mapping.get(x, max_val + 1)), mapping\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Generate new features that might help the model.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify numeric columns for feature engineering\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Remove target if it exists in numeric_cols\n",
    "    if 'Depression' in numeric_cols:\n",
    "        numeric_cols.remove('Depression')\n",
    "    \n",
    "    # Create interaction features for important numeric columns\n",
    "    # This will be helpful if there are relationships between variables\n",
    "    if len(numeric_cols) >= 2:\n",
    "        for i in range(len(numeric_cols)):\n",
    "            for j in range(i+1, len(numeric_cols)):\n",
    "                col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "                # Multiplication interaction\n",
    "                df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "                # Division interaction (with error handling)\n",
    "                df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-5)  # Avoid division by zero\n",
    "                # Addition\n",
    "                df[f'{col1}_plus_{col2}'] = df[col1] + df[col2]\n",
    "                # Subtraction\n",
    "                df[f'{col1}_minus_{col2}'] = df[col1] - df[col2]\n",
    "    \n",
    "    # Square and cube transformations for numeric columns\n",
    "    for col in numeric_cols:\n",
    "        df[f'{col}_squared'] = df[col] ** 2\n",
    "        df[f'{col}_cubed'] = df[col] ** 3\n",
    "        df[f'{col}_log'] = np.log1p(np.abs(df[col]) + 1e-5)  # Log transform (with handling for zeros and negatives)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, is_train=True, encoders=None, scaler=None, feature_selector=None):\n",
    "    # Create a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Store target variable if exists\n",
    "    target = None\n",
    "    if 'Depression' in df.columns and is_train:\n",
    "        target = df['Depression'].copy()\n",
    "    \n",
    "    # Drop ID and Name columns\n",
    "    cols_to_drop = ['id', 'Name']\n",
    "    if not is_train and 'Depression' in df.columns:\n",
    "        # For test data, also drop the Depression column if it exists\n",
    "        cols_to_drop.append('Depression')\n",
    "    df = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)\n",
    "    \n",
    "    # Initialize encoders dictionary if training\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        df[column], mapping = encode_categorical(\n",
    "            df[column].astype(str),\n",
    "            is_train=is_train,\n",
    "            mapping=encoders.get(column)\n",
    "        )\n",
    "        if is_train:\n",
    "            encoders[column] = mapping\n",
    "    \n",
    "    # Handle missing values in numeric variables\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        if is_train:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            imputer.fit(df[numeric_columns])\n",
    "        df[numeric_columns] = imputer.transform(df[numeric_columns])\n",
    "    \n",
    "    # Create additional features\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Standardize features\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "    \n",
    "    # Apply feature selection if needed and if in training mode\n",
    "    if is_train and feature_selector is None:\n",
    "        # Train a basic model for feature selection\n",
    "        temp_model = XGBClassifier(n_estimators=100, random_state=42)\n",
    "        temp_model.fit(scaled_df, target)\n",
    "        \n",
    "        # Select important features\n",
    "        feature_selector = SelectFromModel(temp_model, threshold='median')\n",
    "        feature_selector.fit(scaled_df, target)\n",
    "    \n",
    "    if feature_selector is not None:\n",
    "        feature_mask = feature_selector.get_support()\n",
    "        selected_features = scaled_df.columns[feature_mask]\n",
    "        scaled_df = scaled_df[selected_features]\n",
    "    \n",
    "    # Add back the target for training data\n",
    "    if is_train and target is not None:\n",
    "        scaled_df['Depression'] = target\n",
    "    \n",
    "    return scaled_df, encoders, scaler, feature_selector\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Optuna objective function for XGBoost hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    # XGBoost parameters to optimize\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'early_stopping_rounds':5,\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              \n",
    "              verbose=False)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "def train_optimized_model(train_data):\n",
    "    # Separate features and target\n",
    "    X = train_data.drop('Depression', axis=1)\n",
    "    y = train_data['Depression']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Hyperparameter optimization with Optuna\n",
    "    print(\"Optimizing hyperparameters...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=5)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Train XGBoost with best parameters\n",
    "    xgb_model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        **best_params,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train LightGBM model for ensemble\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    \n",
    "    # Create voting ensemble\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', xgb_model),\n",
    "            ('lgb', lgb_model)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    # Train the ensemble on the full training data\n",
    "    print(\"Training ensemble model...\")\n",
    "    ensemble_model.fit(X, y)\n",
    "    \n",
    "    # Evaluate model\n",
    "    val_predictions = ensemble_model.predict(X_val)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions):.4f}\")\n",
    "    \n",
    "    # Cross-validation to get a more robust estimate\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_cv_train, X_cv_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        cv_model = XGBClassifier(**best_params, random_state=42)\n",
    "        cv_model.fit(X_cv_train, y_cv_train)\n",
    "        \n",
    "        y_cv_pred = cv_model.predict(X_cv_val)\n",
    "        cv_score = accuracy_score(y_cv_val, y_cv_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "    \n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV accuracy: {np.mean(cv_scores):.4f}\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv('train.csv')\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "        \n",
    "        # Print initial column names for debugging\n",
    "        print(\"\\nTraining data columns:\", train_data.columns.tolist())\n",
    "        print(\"Test data columns:\", test_data.columns.tolist())\n",
    "        \n",
    "        # Preprocess training data\n",
    "        print(\"\\nPreprocessing data with advanced feature engineering...\")\n",
    "        processed_train, encoders, scaler, feature_selector = preprocess_data(train_data, is_train=True)\n",
    "        \n",
    "        # Train optimized model\n",
    "        print(\"\\nTraining optimized model...\")\n",
    "        model = train_optimized_model(processed_train)\n",
    "        \n",
    "        # Process test data using the same transformations\n",
    "        processed_test, _, _, _ = preprocess_data(\n",
    "            test_data, \n",
    "            is_train=False, \n",
    "            encoders=encoders,\n",
    "            scaler=scaler,\n",
    "            feature_selector=feature_selector\n",
    "        )\n",
    "        \n",
    "        # Ensure columns match between train and test\n",
    "        train_cols = processed_train.drop('Depression', axis=1).columns\n",
    "        if not all(col in processed_test.columns for col in train_cols):\n",
    "            missing_cols = [col for col in train_cols if col not in processed_test.columns]\n",
    "            print(f\"Warning: Missing columns in test data: {missing_cols}\")\n",
    "            # Add missing columns with zeros\n",
    "            for col in missing_cols:\n",
    "                processed_test[col] = 0\n",
    "        \n",
    "        processed_test = processed_test[train_cols]\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_predictions = model.predict(processed_test)\n",
    "        \n",
    "        # Create submission file\n",
    "        submission = pd.DataFrame({\n",
    "            'id': test_data['id'],\n",
    "            'Depression': test_predictions\n",
    "        })\n",
    "        submission.to_csv('submission6.csv', index=False)\n",
    "        print(\"\\nSubmission file created: submission_optimized.csv\")\n",
    "        \n",
    "        # Print feature importance (if available)\n",
    "        if hasattr(model, 'named_estimators_') and hasattr(model.named_estimators_['xgb'], 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': train_cols,\n",
    "                'importance': model.named_estimators_['xgb'].feature_importances_\n",
    "            })\n",
    "            print(\"\\nTop 10 Most Important Features:\")\n",
    "            print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "       S print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nDetailed error information:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c228766-d149-4924-8bab-eef5f9ca617b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
