{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815114f9-37ee-42e9-89ac-9c4797422d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.47728\n",
      "[100]\tvalidation_0-logloss:0.32039\n",
      "[200]\tvalidation_0-logloss:0.25159\n",
      "[300]\tvalidation_0-logloss:0.21537\n",
      "[400]\tvalidation_0-logloss:0.19471\n",
      "[500]\tvalidation_0-logloss:0.18240\n",
      "[600]\tvalidation_0-logloss:0.17474\n",
      "[700]\tvalidation_0-logloss:0.16975\n",
      "[800]\tvalidation_0-logloss:0.16646\n",
      "[900]\tvalidation_0-logloss:0.16411\n",
      "[1000]\tvalidation_0-logloss:0.16231\n",
      "[1100]\tvalidation_0-logloss:0.16088\n",
      "[1200]\tvalidation_0-logloss:0.15976\n",
      "[1300]\tvalidation_0-logloss:0.15887\n",
      "[1400]\tvalidation_0-logloss:0.15820\n",
      "[1500]\tvalidation_0-logloss:0.15761\n",
      "[1600]\tvalidation_0-logloss:0.15715\n",
      "[1700]\tvalidation_0-logloss:0.15678\n",
      "[1800]\tvalidation_0-logloss:0.15645\n",
      "[1900]\tvalidation_0-logloss:0.15616\n",
      "[2000]\tvalidation_0-logloss:0.15595\n",
      "[2100]\tvalidation_0-logloss:0.15577\n",
      "[2200]\tvalidation_0-logloss:0.15561\n",
      "[2300]\tvalidation_0-logloss:0.15546\n",
      "[2400]\tvalidation_0-logloss:0.15533\n",
      "[2500]\tvalidation_0-logloss:0.15524\n",
      "[2600]\tvalidation_0-logloss:0.15520\n",
      "[2700]\tvalidation_0-logloss:0.15514\n",
      "[2800]\tvalidation_0-logloss:0.15509\n",
      "[2900]\tvalidation_0-logloss:0.15507\n",
      "[3000]\tvalidation_0-logloss:0.15502\n",
      "[3044]\tvalidation_0-logloss:0.15502\n",
      "\n",
      "Validation Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     22986\n",
      "           1       0.84      0.81      0.82      5154\n",
      "\n",
      "    accuracy                           0.94     28140\n",
      "   macro avg       0.90      0.89      0.89     28140\n",
      "weighted avg       0.94      0.94      0.94     28140\n",
      "\n",
      "Validation Accuracy: 0.9366\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "   feature  importance\n",
      "0        0    0.382072\n",
      "14      14    0.086622\n",
      "15      15    0.063248\n",
      "13      13    0.062630\n",
      "11      11    0.053367\n",
      "20      20    0.039607\n",
      "3        3    0.038325\n",
      "10      10    0.021903\n",
      "16      16    0.021538\n",
      "4        4    0.018266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [09:27:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as: depression_prediction_xgboost.model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def encode_categorical(series, is_train=True, mapping=None):\n",
    "    if is_train:\n",
    "        unique_values = series.unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    max_val = max(mapping.values()) if mapping else 0\n",
    "    return series.map(lambda x: mapping.get(x, max_val + 1)), mapping\n",
    "\n",
    "def preprocess_data(df, is_train=True, encoders=None, scaler=None, pca=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate target variable if exists\n",
    "    target = None\n",
    "    if 'Depression' in df.columns:\n",
    "        target = df['Depression']\n",
    "        df = df.drop('Depression', axis=1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['id', 'Name']\n",
    "    df = df.drop([col for col in cols_to_drop if col in df.columns], axis=1)\n",
    "    \n",
    "    # Initialize encoders and scalers if training\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "        scaler = StandardScaler()\n",
    "        pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        df[column], mapping = encode_categorical(df[column].astype(str), is_train=is_train, mapping=encoders.get(column))\n",
    "        if is_train:\n",
    "            encoders[column] = mapping\n",
    "    \n",
    "    # Handle numeric variables\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # 1. Polynomial Features\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    poly_features = poly.fit_transform(df[numeric_columns])\n",
    "    poly_columns = [f'poly_{i}' for i in range(poly_features.shape[1])]\n",
    "    df_poly = pd.DataFrame(poly_features, columns=poly_columns)\n",
    "    df = pd.concat([df, df_poly], axis=1)\n",
    "    \n",
    "    # 2. Clustering Features\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    df['cluster'] = kmeans.fit_predict(df[numeric_columns])\n",
    "    \n",
    "    # 3. Ratio Features\n",
    "    if {'SleepHrs', 'WorkHrs'}.issubset(df.columns):\n",
    "        df['Sleep_Work_Ratio'] = df['SleepHrs'] / (df['WorkHrs'] + 1)\n",
    "    \n",
    "    # 4. Standard Scaling\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # 5. Dimensionality Reduction\n",
    "    df_reduced = pca.fit_transform(df_scaled)\n",
    "    df_final = pd.DataFrame(df_reduced)\n",
    "    \n",
    "    # Reattach target variable if present\n",
    "    if target is not None:\n",
    "        df_final['Depression'] = target.values\n",
    "    \n",
    "    return df_final, encoders, scaler, pca\n",
    "\n",
    "def train_model(train_data):\n",
    "    X = train_data.drop('Depression', axis=1)\n",
    "    y = train_data['Depression']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.005,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'n_estimators': 5000,\n",
    "        'verbosity': 0,\n",
    "        'early_stopping_rounds': 50,\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Validation Accuracy: {accuracy_score(y_val, val_predictions):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    \n",
    "    processed_train, encoders, scaler, pca = preprocess_data(train_data, is_train=True)\n",
    "    model = train_model(processed_train)\n",
    "    processed_test, _, _, _ = preprocess_data(test_data, is_train=False, encoders=encoders, scaler=scaler, pca=pca)\n",
    "    \n",
    "    train_cols = processed_train.drop('Depression', axis=1).columns\n",
    "    processed_test = processed_test[train_cols]\n",
    "    test_predictions = model.predict(processed_test)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': test_data['id'], 'Depression': test_predictions})\n",
    "    submission.to_csv('submission_xgboost.csv', index=False)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'feature': train_cols, 'importance': model.feature_importances_})\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "    \n",
    "    model.save_model('depression_prediction_xgboost.model')\n",
    "    print(\"\\nModel saved as: depression_prediction_xgboost.model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63569e75-26e6-4a8e-9a6e-51a37fadcf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
